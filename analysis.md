Thanks for the details. I’ll analyze how to structure your DiPeO monorepo project using Python and GraphQL backend with DI containers, focusing on organizing domain logic for safe local use and shared usage between server and CLI. I’ll also look at best practices for managing containers and dependencies in this context.

I’ll get back to you with a structured breakdown and recommendations shortly.


# Structuring DiPeO’s Monorepo: Domain Logic, DI Containers, and Dependencies

## Monorepo Package Organization

Your monorepo approach of separating **DiPeO** as a core library and having distinct apps (front-end, back-end, CLI) is sound. In this setup, the **`dipeo`** package acts as an umbrella for all core logic (domain, application, etc.), while the front-end (React UI), GraphQL back-end, and CLI are just different interfaces on top of that logic. In fact, the repository shows that multiple sub-packages (core, domain, diagram, application, infra, container) were merged under the single `dipeo` umbrella for unified access. This means all business logic and shared code live in one place, making it easier to maintain and reuse across different interfaces.

**Key points for monorepo structure:**

* **Core Logic in One Package:** Keep the domain and processing logic in the `dipeo` package. This module should contain your business rules, models, and use-case implementations. It appears you’ve already done this by moving things like `dipeo.core`, `dipeo.domain`, `dipeo.application`, etc., under one roof. This ensures a single source of truth for logic that both the server and CLI can call.
* **Separate Interface Apps:** The front-end (React) and back-end (GraphQL with FastAPI) and any CLI tools should be thin layers that depend on the `dipeo` package. They handle **presentation and input/output** (HTTP requests, CLI arguments, UI interactions) but delegate the heavy lifting to `dipeo`. In practice, your **`apps/server`** code (GraphQL resolvers) should call into `dipeo` services or use-cases to perform operations, and the **`apps/cli`** should do likewise for command-line commands.
* **Shared Interfaces/Protocols:** Define clear interfaces or protocols for any external interactions (e.g. file storage, LLM API, database) in the core, and implement them in the outer layers. This way, `dipeo` can define what it **needs** (as abstract ports), and the server/CLI can provide concrete implementations (adapters) without changing the core logic. This aligns with *Hexagonal* or *Clean Architecture* principles, where the domain is inner and frameworks/UI are outer.

Overall, your monorepo pattern is on the right track: a central **domain logic package** and multiple deployment targets (server, CLI, etc.) that use it. The goal is to ensure the **domain layer is independent** of any specific interface. In Clean Architecture terms, *“the domain layer contains the business logic of the system and is the core, independent of other layers”*. The application layer (use cases/workflows) uses the domain, and the infrastructure layer (GraphQL server, file system, etc.) is outside and can be swapped without affecting domain logic.

## Domain Logic as Services vs. Plain Functions

You observed that “all the domain logic is just being made into services and then injected into the container.” Indeed, in the current implementation, many domain operations are encapsulated in service classes that are registered with a dependency injection (DI) container. For example, the code defines providers for things like `APIKeyDomainService`, `LLMInfraService`, file service, etc., in the container setup. This means instead of calling a function directly, you instantiate a service (possibly a class) that handles that domain logic, and the DI container provides it wherever needed.

**Is this the right pattern?** It’s one valid pattern – using **Dependency Injection** to decouple creation of components from their use. The core idea is that each piece of domain logic (API key management, file operations, text processing, etc.) is in its own class (a “service”) which declares what it needs (dependencies) and focuses on one area of logic. Then a DI container or factory **injects** those dependencies when constructing the service. This pattern can improve modularity and testability: *“Dependency injection is a technique for decoupling components… passing dependencies to a class instead of creating them internally. This makes it easier to test in isolation and replace dependencies with mocks”*. In your case, domain services like `DBOperationsDomainService` or `APIIntegrationDomainService` accept needed collaborators (file storage, validators, etc.) via their constructors, which you are indeed doing (e.g. passing `file_service` and `validation_service` into `DBOperationsDomainService`). This is good design — the domain logic *receives* whatever services or adapters it needs, rather than looking them up or hard-coding them.

However, using a DI container to assemble **every** domain service can become verbose or “messy” if not organized well. It looks like you have a `Container` class in `dipeo.container` that creates all these services and a `ServerContainer` subclass that overrides some for the server environment. This central container approach is essentially acting as the “composition root” of your application — the place where all pieces are wired together. The benefit is that you can swap out implementations easily (for example, use a dummy `MinimalStateStore` when not running the server, or use an environment-based API key store vs. a file store). The downside is a lot of boilerplate in one place and potentially a **Service Locator** style usage (the `UnifiedServiceRegistry` you have is essentially a registry of services by name). If everything is just fetched from a container globally, it can reduce clarity about what each part actually needs.

**Best practice:** The pattern of separating domain logic into services is fine if you maintain clear boundaries: each service has a single responsibility and well-defined interface. But you don’t *have* to inject everything via a global container. An alternative is to manually pass dependencies when calling domain functions or constructing use-case objects. In fact, your CLI `LocalAppContext` is doing something like this manually – it creates the needed services and assigns them to itself without using the container framework. Both approaches (manual injection vs. container) achieve the same goal of giving domain logic the resources it needs. The right choice depends on complexity and personal/team preference:

* For **explicitness and simplicity**, manual injection (creating the objects in code and passing them in) can be easier to follow. It avoids magic and global state. Your CLI context shows this: you explicitly initialize `EnvironmentAPIKeyService`, then `LLMInfraService(self._api_key_service)`, then others, and finally pass them into a `LocalExecutionService(self)`. Domain logic still “just receives what it needs” – exactly what you want – but without a lot of container indirection.
* For **scalability and consistency** across different environments, a DI container framework can reduce duplicate wiring code. For example, your server container uses the `dependency_injector` library to declaratively wire up providers for each service. This avoids having to repeat instantiation logic everywhere; the container can also enforce certain lifetimes (singletons vs factories) and check interface compliance (you have `_validate_protocol_compliance` doing this). If you have many services and complex dependency graphs, a container keeps that assembly in one place.

**Recommendation:** It’s perfectly acceptable to use the container pattern **if you keep the domain layer agnostic of it** (i.e. domain services are plain classes, unaware of *how* they’re injected). Your code already follows this – domain classes just define `__init__` with required params, they don’t import the container. So the container is an implementation detail of the application setup, which is good. If the current container code is “messy,” consider refactoring it for clarity rather than abandoning DI altogether. For instance, group related wiring (maybe have sub-containers or factory functions for certain families of services), or use config files for the container if the library supports it. The key is to make it easier to see **what** is being provided to each domain service without scrolling through hundreds of lines. Even using a bit of commenting or grouping in the container can help.

On the other hand, if you feel DI via a framework is overkill for your project’s size, you could simplify: e.g., create a simple **factory module** that instantiates the needed services for a given context (similar to your `LocalAppContext.initialize_for_local`). This would remove the extra layer of `providers.Singleton(...)` syntax and just use normal Python objects. The trade-off is losing the automatic features (like ensuring singletons or auto-wiring into GraphQL resolvers, etc., which you currently do with `container.wire(...)` in FastAPI). Evaluate whether those features are providing enough benefit to justify the complexity.

## Cleanly Separating Domain, Application, and Infrastructure

To achieve the ideal pattern – where “dipeo contains the overall logic and can be run from the server or CLI” – you should structure the code into clear layers, which it mostly is. Think in terms of Clean Architecture or Hexagonal Architecture:

* **Domain Layer (dipeo.domain, dipeo.core):** This is your core logic – definitions of data models, entities, and business rules. For DiPeO, this would include things like the Diagram model, People/Organization models, and any pure domain services or calculations. This layer should have **no dependency on frameworks or external tech**. It may define abstract interfaces (protocols) for things it needs (e.g. a storage interface, an LLM interface), but it doesn’t implement file access or API calls itself. The domain layer is central and independent: *“the domain layer contains business logic and is the core of the system, independent of other layers”*.

* **Application Layer (dipeo.application):** This layer orchestrates domain logic to fulfill use cases. Here you’d have things like your **Execution Engine or UseCase classes** (e.g. `ExecuteDiagramUseCase`, `PrepareDiagramForExecutionUseCase`). These coordinate multiple domain services or entities to perform a high-level task (like executing a diagram, which might involve validating it, reading files, calling LLM, etc.). The application layer can depend on the domain layer (it uses domain services/entities), but remains independent of infrastructure. It typically interacts with **interfaces** for infrastructure (for example, it might call a `LLMService` interface, not a specific API client directly). In your code, some of these use-case classes currently reside in `dipeo.domain.domains.execution.services`, but as your own comment in the code notes, ideally they belong in the application layer to avoid domain depending on EngineFactory. Strive to keep use-case coordination logic at this level, separate from both raw domain and from details of frameworks.

* **Infrastructure Layer (dipeo.infra and external modules):** This includes implementations of interfaces for databases, file systems, network calls, external APIs, etc. In DiPeO, things like `ConsolidatedFileService`, `LLMInfraService` (for calling LLM APIs), or `NotionAPIService` are infrastructure. These know about files, HTTP, environment variables, etc. The infrastructure layer can depend on domain (e.g. a FileService might know about a Diagram format), but the domain layer should not depend on these concretes. By separating them, you could replace one with another (say swap out a file-based storage with an in-memory one for tests) easily. Your design already does this to an extent: for example, `EnvironmentAPIKeyService` vs `APIKeyDomainService` – one might be reading OS env vars (infra detail), while the domain service defines higher-level logic for API keys.

* **Presentation Layer (Apps – server, CLI, front-end):** This is the outermost layer. It includes anything related to delivering the functionality to a user or another system. The FastAPI/GraphQL server and the CLI code are here. They should **not contain business logic**; they only handle things like parsing a GraphQL request or CLI argument, calling the appropriate `dipeo` application service, and then formatting the result (or streaming updates to the client, etc.). For instance, a GraphQL resolver might call something like `container.execution_service().execute_diagram(...)` and then return the results. The CLI might call `LocalExecutionService.execute_diagram(...)` and stream the events to the console. If these layers start containing significant logic, consider pushing that logic down into `dipeo` where it can be reused by both interfaces.

Your goal of *“dipeo contains the overall logic and \[the app] just receives the processed results”* is exactly what these layers aim for. The front-end or CLI shouldn’t need to know **how** a diagram is executed or how a query is processed – they just display outcomes or pass inputs. By adhering to the dependency rule (outer layers depend on inner layers, but not vice versa), you can run the same core logic anywhere. For example, both server and CLI execution ultimately use the **same Engine and handlers** under the hood. In your code, the **UnifiedServiceRegistry** is one mechanism that makes this possible – it allows the engine and node handlers to get whatever service (LLM, file, etc.) they need without caring if it’s running in server mode or local mode. In server mode, you inject the services via constructor (the DI container populates the registry with the real implementations); in local mode, you build a registry from the `LocalAppContext`. This is essentially the *Ports and Adapters* idea: the core logic (engine) talks to an abstract “service registry” port, which is adapted either by the server container or the CLI context.

**Actionable structuring tips:**

* **Keep domain logic self-contained:** If you find any domain code reaching out to global state or performing I/O, refactor it to request that functionality via an interface. For example, if a domain service needs to read a file, give it a file repository interface and implement that in `dipeo.infra`. This way, in one environment the file repo could hit a real filesystem, and in another it could be an in-memory stub.
* **Minimize cross-layer knowledge:** The domain layer should have no idea if it’s being called from a CLI or a web server. It just knows how to execute the command it’s given. Conversely, the CLI or server should not duplicate domain logic – they shouldn’t, say, parse diagram structures or enforce business rules; they call into `dipeo` to do that. If you notice duplication, consider exposing a function or service in `dipeo` to handle that.
* **Use the application layer as the API:** Consider providing a clear API in the `dipeo` package that the outside world uses. For example, a function or class method like `ExecutionEngine.run_diagram(diagram_input)` or a facade that aggregates common operations. Right now, it looks like the main entry point is the `ExecuteDiagramUseCase` or `ExecutionService`. If that’s the case, make sure it’s easy for the server or CLI to invoke. This might simply be ensuring the DI container exposes an `execution_service` provider (which you have) or providing a convenience function. The easier it is to call into the core, the thinner your outer layers remain.

By following these practices, you essentially implement the Clean Architecture style: *“organizing into layers makes the system easier to understand, maintain, and test”*. Each layer has a clear role, and changing one (say swapping out your GraphQL transport for a REST API) won’t ripple unintended changes into the others.

## Running DiPeO from Server vs CLI

One of your concerns is how to structure things so that the same **DiPeO logic runs in both a server and a CLI context**. You’re already doing a form of this by using the DI container differently in each scenario. Let’s break down how to manage that cleanly:

* **Environment-Specific Configuration:** The core logic should be parameterized by its environment. In practice, this means when you initialize the application, you provide different implementations or settings depending on context. For example, in server mode you use a persistent `state_store` and a live `MessageRouter` for streaming results to clients, whereas in local CLI mode you might use an in-memory or “minimal” state store and no live message router. Your code reflects this: `_import_state_store` in the base container tries to import the server’s persistent store, and falls back to a `MinimalStateStore` if that’s not available (i.e. running in CLI). Similarly for `_import_message_router`. This approach is fine for a quick solution, but it can be made more explicit. Instead of hiding the decision in try/except, you could **pass a flag or configuration** to the initialization function. For instance, have something like `dipeo.container.create_container(mode="server")` vs `mode="cli"`, which then deliberately chooses the appropriate providers. This makes the setup clearer than implicit imports. Your current approach with a separate `ServerContainer` subclass is one way: it clearly overrides certain providers with server-specific ones (like using `NotionAPIService` in server vs `None` in base). That’s a good separation of concerns. You might do something analogous for CLI (perhaps a `CLIContainer` or simply use the base container which already defaults to minimal implementations).

* **Unified Service Interface:** Both the server and CLI ultimately need to invoke the same *use case* (execute a diagram, retrieve data, etc.). Provide a unified interface for that. It could be the `ExecuteDiagramUseCase.execute_diagram()` method, which both the GraphQL resolver and CLI call. In your design, the **server** likely calls `container.execution_service().execute_diagram(...)` to stream execution events, whereas the **CLI** uses `LocalExecutionService.execute_diagram(...)` (which yields events for the console). Notice these two share a lot of logic – indeed, both create an engine via `EngineFactory` and yield updates, differing only in observers like `StreamingObserver` vs `LocalUpdateCollector`. This is a sign that the core execution mechanism is shared, which is good. To further unify, you could ensure both server and CLI use a common abstraction for execution (perhaps a single `ExecutionEngine` class with different configuration). But even as is, it’s acceptable: you’ve effectively implemented two **facades** over the same engine. Just ensure they remain thin. If you find yourself duplicating code between `ExecuteDiagramUseCase` (server) and `LocalExecutionService` (CLI), consider refactoring the common parts into a function inside `dipeo` that both call. For example, both do `engine.execute(diagram_obj, execution_id, options, interactive_handler)` in a background task and then stream results and. That logic could potentially live in one place to avoid divergence.

* **Dependency Injection per Context:** Using a DI container doesn’t mean you must use *one global container for all contexts*. In fact, you should treat each running instance separately. You already have a global `ServerContainer` (set in `app_context.py` on server startup) and effectively a separate setup for CLI. This is good. Each entry point (server or CLI) can initialize its needed dependencies at startup. This practice is often called having a **composition root** for each application. For FastAPI/GraphQL, the composition root is likely in the startup events (you call `initialize_container()` and `init_server_resources()` on app startup). For CLI, the composition happens in the `LocalAppContext.initialize_for_local()` before running a command. Keep these initialization steps well-contained. Document clearly how to bootstrap the environment for each case. This way, running DiPeO in a new context (imagine a desktop app or a scheduled job) is just a matter of providing a new composition that supplies the appropriate dependencies (maybe an in-memory or mocked set for tests, etc.).

To directly answer *“how I should manage containers and dependencies”*: Manage them **per environment**, and keep them as simple as needed. For example, the server might need a more complex container (with async initialization of resources like DB connections, message broker, etc.), while the CLI can use a lightweight setup. That’s okay. You can still share a lot: the majority of your providers in `Container` (base) can be reused, with just a few overrides in `ServerContainer` for specifics. If the current inheritance approach is messy, an alternative is to not subclass but rather compose the configurations. For instance, you could have a function that builds a base container, then a separate function that tweaks it for server (e.g. providing actual NotionAPIService). Subclassing as you did is fine as long as it’s clear which providers are different.

One thing to watch out: **Global State.** Using a container often introduces a global object (like your `_container` singleton). While convenient, be careful that global state is initialized exactly once and properly cleaned up. Your code calls `shutdown_server_resources` on shutdown which is good. For thread-safety or multi-instance scenarios (like if you ever had more than one container in memory), you’d need to avoid globals. In the current local use case, it’s likely not an issue (one user, one server process). Just keep it in mind if scaling up. In tests, for example, you might want to instantiate a fresh container per test to avoid cross-test interference.

Finally, regarding **feeling safe for local use**: since you deploy it the same way locally and in any environment (monorepo with all pieces together), it indeed should feel safe – you’re essentially running the same code paths. The CLI and server just exercise them differently. By decoupling through DI, you’ve ensured that *“designing components to reduce interdependence makes it easier to replace or modify parts without affecting the whole system”*. This decoupling is what allows that feeling of safety: you can trust that changes in, say, the GraphQL API layer won’t break the core diagram execution logic, as long as the interface contract is the same.

## Conclusion and Suggested Pattern

**Summary:** You’re essentially working toward a **Clean Architecture** style solution, which is a great choice. The domain logic lives in the `dipeo` package, and both the server and CLI interface adapt to it. The use of services and a DI container is meant to enforce that separation – domain services get what they need injected, rather than hard-coding assumptions about the environment. This pattern is correct in principle. As the guidelines say, the domain (business logic) should be isolated and your outer layers (GraphQL, CLI, React UI) should depend on it, not vice versa.

**Improvements:** To implement the “best pattern” for your case, consider these refinements:

* **Clarify Layer Boundaries:** Make sure each module in `dipeo` has a clear responsibility (core/domain vs application vs infra). This will naturally reduce the confusion around where things should be initialized. For instance, if `ExecuteDiagramUseCase` is in the application layer, then the container (which is part of infrastructure setup) knows it needs to supply infrastructure implementations *to* that use case. You might then rename things like `...DomainService` vs `...UseCase` to distinguish domain-pure services (which might be algorithms or calculations) from use-case coordinators (which tie multiple services together).
* **Simplify Dependency Wiring:** If the current DI container feels like overkill, try adopting a hybrid approach: use simple factories for small cases and container for complex ones. You don’t have to inject absolutely everything. For example, if a domain service is trivial to construct (no expensive resources), you might just instantiate it on the fly where needed instead of making it a long-lived singleton in the container. Reserve the container for cross-cutting concerns or expensive singletons (like database connections, API clients, etc.). This reduces container bloat.
* **Consistency Between CLI and Server:** To avoid divergence, you could reuse the same definitions. Perhaps the CLI could even use the container in a trimmed form: e.g., instantiate the base `Container` and call its `init_resources()` for minimal mode. Currently the CLI bypasses the container and manually builds `LocalExecutionService`. That works, but it’s parallel to what the container does. If maintaining two ways becomes cumbersome, unify them – maybe have the CLI also leverage `UnifiedServiceRegistry.from_context` or have it call a common initializer. On the other hand, if the container will always be heavier, keeping the CLI light might be intentional. Just ensure any changes in how services are wired in one path are reflected in the other as needed. Writing good documentation (even just in code comments) about how each environment is set up will help future you or other developers.

In essence, aim for an architecture where **DiPeO (the package) is your core**, and it defines what it needs abstractly. The **Server and CLI are just delivery mechanisms** providing those needs and then presenting results. You’re already doing that; the remaining work is mostly cleanup and clarity – removing any accidental complexity so that the pattern becomes obvious. Adopting a Clean Architecture mindset (domain vs application vs infrastructure separation) will guide these decisions. It seems you’re very close to that ideal.

By structuring things this way, you ensure maximum flexibility. Today it’s GraphQL and CLI; tomorrow it could be a different interface or a background scheduler – and you’d be able to plug it in with minimal changes to the core. The DI approach, when managed properly, will give you the confidence that each piece of the system can evolve or be tested in isolation. Remember, the ultimate litmus test of “best pattern” is **ease of understanding and change**. If a new developer (or future you) can quickly grasp how a request flows from the UI/CLI down to the domain and back, and can modify one part (say swap out the LLM provider) without fear, then you’ve achieved your goal.

All things considered, keep the domain logic self-contained and feed it the services it needs (as you said, *“receive whatever services it needs and handle everything”*). Use the container or any DI mechanism as a tool to assemble these parts at the application’s entry point. This decoupled design will pay off with safer changes and a more maintainable codebase moving forward. Good luck, and happy architecting!

**Sources:**

* ThinhDA, *“Crafting Maintainable Python Applications with DDD and Clean Architecture”* – Discusses layering (Domain, Application, Infrastructure) and dependency injection in Python.
* *DiPeO Repository (dev branch)* – Implementation of DiPeO’s DI container and services, showing how domain services are provided and injected, and how a unified service registry allows both server and CLI usage.
* Ijash (DEV.to), *“Decoupling Dependencies in Clean Architecture”* – Explains the benefit of reducing interdependence among components for flexibility.
