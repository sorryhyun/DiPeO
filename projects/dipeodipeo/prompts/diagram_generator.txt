Generate a DiPeO light format diagram based on the following workflow description:

{{workflow_description}}

{% if sample_test_data %}
SAMPLE TEST DATA PROVIDED:
{{sample_test_data}}

Use this test data to understand the expected input format and structure your diagram accordingly.
{% endif %}

CRITICAL REQUIREMENTS:

1. CONTENT TYPE RULES:
   - NEVER use 'empty' as a content_type - it provides no value
   - Use 'raw_text' for plain text, CSV, or unstructured data
   - Use 'object' for structured data (dictionaries, JSON objects, parsed data)
   - Use 'conversation_state' only for LLM conversation contexts
   - ALWAYS specify content_type for connections carrying data

2. DIAGRAM STRUCTURE:
   - Always start with a 'start' node at position {x: 100, y: 200}
   - End with an 'endpoint' node for file output or final results
   - Position nodes with x-coordinates incrementing by 200-300 for readability
   - Keep y-coordinates consistent for linear flow (200 for main path)
   - Use y: 400 for alternative branches in condition nodes

3. NODE SELECTION (NO SUB-DIAGRAMS):
   - Use 'db' nodes for file I/O operations (reading/writing files)
   - Use 'code_job' nodes for ALL data processing, validation, transformation
   - Use 'person_job' nodes only when LLM analysis is absolutely required
   - Use 'condition' nodes for branching logic
   - Use 'endpoint' nodes to save final results
   - DO NOT use 'sub_diagram' nodes - implement parallel processing with code_job and asyncio instead

4. DATA FLOW:
   - Always label connections with meaningful variable names
   - Connection labels become variable names in downstream nodes
   - Use descriptive labels like 'raw_data', 'parsed_records', 'validation_results'
   - Specify appropriate content_type for EVERY connection with data

5. CODE_JOB BEST PRACTICES:
   - Keep code concise and focused on single responsibility
   - Use Python for data processing
   - Always set result variable or use return statement
   - Access input variables by their connection label names
   - For parallel processing, use asyncio within code_job:
     ```python
     import asyncio
     async def process_item(item):
         # Process individual item
         return processed_item
     
     async def main():
         tasks = [process_item(item) for item in input_data]
         results = await asyncio.gather(*tasks)
         return results
     
     result = asyncio.run(main())
     ```

6. FILE OPERATIONS:
   - For reading multiple files, use glob patterns with glob: true
   - For CSV files, parse them in a code_job node after reading as raw_text
   - Use relative paths from project root (e.g., "files/data/input.csv")
   - Always validate file contents before processing

7. ERROR HANDLING:
   - Add validation nodes after data loading
   - Use condition nodes to handle error cases
   - Include try-except blocks in code_job nodes
   - Log errors for debugging

EXAMPLE PATTERNS:

For CSV processing without sub_diagram:
```yaml
- label: load_csv
  type: db
  props:
    operation: read
    sub_type: file
    source_details: files/data/input.csv
    format: text

- label: parse_csv
  type: code_job
  props:
    language: python
    code: |
      import csv
      from io import StringIO
      
      reader = csv.DictReader(StringIO(csv_content))
      records = list(reader)
      result = {"records": records, "count": len(records)}

connections:
  - {from: load_csv, to: parse_csv, label: csv_content, content_type: raw_text}
```

For parallel processing without sub_diagram:
```yaml
- label: process_parallel
  type: code_job
  props:
    language: python
    code: |
      import asyncio
      
      async def process_record(record):
          # Your processing logic here
          await asyncio.sleep(0.01)  # Simulate async work
          return {**record, "processed": True}
      
      async def main():
          tasks = [process_record(r) for r in input_records]
          results = await asyncio.gather(*tasks)
          return results
      
      result = asyncio.run(main())
```

For validation with proper content types:
```yaml
- label: validate_data
  type: code_job
  props:
    language: python
    code: |
      valid_records = []
      invalid_records = []
      
      for record in input_data.get("records", []):
          if all(field in record for field in required_fields):
              valid_records.append(record)
          else:
              invalid_records.append(record)
      
      result = {
          "valid": valid_records,
          "invalid": invalid_records,
          "is_valid": len(invalid_records) == 0
      }

connections:
  - {from: parse_csv, to: validate_data, label: input_data, content_type: object}
  - {from: validate_data, to: process_valid, label: validation_result, content_type: object}
```

Generate a complete, executable diagram following these guidelines. Ensure all node properties are valid, content_types are properly specified (never 'empty'), and the diagram can be run immediately with the dipeo command.