version: light

persons:
  Prompt Designer:
    service: openai
    model: gpt-5-nano-2025-08-07
    api_key_id: APIKEY_52609F
    system_prompt: |
      You are an expert in prompt engineering for code generation.
      Create clear, comprehensive, and effective prompts for frontend code generators.
      Focus on specificity, completeness, and actionable instructions.
      Do not add any suggestions which are not requested, like 'If you want' or something at the end.
      
  Frontend Generator:
    service: openai
    model: gpt-5-nano-2025-08-07
    api_key_id: APIKEY_52609F
    system_prompt: |
      You are an expert frontend developer specializing in React and modern web development.
      Generate production-ready frontend code based on the provided prompts.
      Follow best practices for component structure, state management, and code organization.
      Do not add any suggestions which are not requested, like 'If you want' or something at the end.

      
  Code Evaluator:
    service: openai
    model: gpt-5-nano-2025-08-07
    api_key_id: APIKEY_52609F
    system_prompt: |
      You are a frontend code quality expert.
      Evaluate generated frontend code for correctness, best practices, performance, and maintainability.
      Provide feedback on how the prompt should be improved to generate better code.
      Do not add any suggestions which are not requested, like 'If you want' or something at the end.

nodes:
  - label: Start
    type: start
    position: {x: 50, y: 400}
    props:
      trigger_mode: manual

  - label: Load Config
    type: db
    position: {x: 200, y: 400}
    props:
      operation: read
      sub_type: file
      format: json
      source_details: projects/frontend_enhance/frontend_enhance_config.json

  - label: Initialize State
    type: code_job
    position: {x: 400, y: 400}
    props:
      language: python
      code: |
        import json
        
        # Parse the loaded config
        if isinstance(config, str):
            config_data = json.loads(config)
        else:
            config_data = config
        
        iteration_count = 0
        current_score = 0
        improvements_history = []
        
        result = {
            "history": improvements_history,
            "prompt_type": config_data["prompt_type"],
            "target_framework": config_data["framework"],
            "prompt_requirements": config_data["prompt_requirements"],
            "evaluation_criteria": config_data["evaluation_criteria"],
            "prompt_sections": config_data["prompt_sections"],
        }

  - label: Generate Prompt
    type: person_job
    position: {x: 600, y: 400}
    props:
      person: Prompt Designer
      first_prompt_file: prompt_generator_first.txt
      prompt_file: prompt_generator.txt
      max_iteration: 3
      memory_profile: ONLY_I_SENT

  - label: Generate Frontend Code
    type: person_job
    position: {x: 800, y: 400}
    props:
      person: Frontend Generator
      default_prompt: "{{generated_prompt}}"
      max_iteration: 3
      memory_profile: GOLDFISH

  - label: Save Frontend Code
    type: db
    position: {x: 900, y: 300}
    props:
      operation: write
      sub_type: file
      format: json
      source_details: projects/frontend_enhance/generated/frontend_generator.json

  - label: Evaluate Generated Code
    type: person_job
    position: {x: 1000, y: 400}
    props:
      person: Code Evaluator
      prompt_file: code_evaluator.txt
      max_iteration: 3
      memory_profile: GOLDFISH

  - label: Check Score
    type: code_job
    position: {x: 1200, y: 400}
    props:
      language: python
      code: |
        import re
        # Extract score from code evaluation
        score_match = re.search(r'Score:\s*(\d+)', code_evaluation, re.IGNORECASE)
        if score_match:
            score = int(score_match.group(1))
        
        # Check if ready for production
        ready = "production ready" in code_evaluation.lower() or score >= 80
        
        result = {
            "score": score,
            "ready_for_production": ready,
            "code_evaluation": code_evaluation
        }

  - label: Check Quality Target
    type: condition
    position: {x: 1400, y: 400}
    props:
      condition_type: custom
      expression: checked.ready_for_production == True

  - label: Detect Max Iterations
    type: condition
    position: {x: 1200, y: 600}
    props:
      condition_type: detect_max_iterations

  - label: Stop if Max Iterations
    type: endpoint
    position: {x: 1000, y: 700}
    props:
      file_format: txt
      save_to_file: false

  - label: Extract and Setup App
    type: code_job
    position: {x: 1100, y: 200}
    props:
      language: python
      filePath: projects/frontend_enhance/code/extract_and_setup_app.py
      functionName: main

  - label: Save Enhanced Prompt
    type: db
    position: {x: 1600, y: 400}
    props:
      operation: write
      sub_type: file
      format: json
      source_details: projects/frontend_enhance/generated/results.json

  - label: Save Summary
    type: endpoint
    position: {x: 2000, y: 400}
    props:
      file_format: md
      save_to_file: true
      file_path: projects/frontend_enhance/generated/README.md

connections:
  # Initial flow
  - {from: Start, to: Load Config}
  - {from: Load Config, to: Initialize State, label: config, content_type: object}
  - {from: Initialize State, to: Generate Prompt_first, content_type: object}
  
  # Generation and evaluation flow
  - {from: Generate Prompt, to: Generate Frontend Code, label: generated_prompt, content_type: object}
  - {from: Generate Frontend Code, to: Save Frontend Code, label: generated_code, content_type: object}
  - {from: Generate Frontend Code, to: Extract and Setup App, label: generated_code, content_type: object}
  - {from: Generate Frontend Code, to: Evaluate Generated Code, label: generated_code, content_type: object}
  - {from: Evaluate Generated Code, to: Check Score, label: code_evaluation, content_type: object}
  - {from: Check Score, to: Check Quality Target, label: checked, content_type: object}
  
  # Success path (quality target met)
  - {from: Check Quality Target_condtrue, to: Save Enhanced Prompt, content_type: object}
  - {from: Save Enhanced Prompt, to: Save Summary, label: saved_prompt}
  
  # Check if max iterations reached
  - {from: Check Quality Target_condfalse, to: Detect Max Iterations}
  - {from: Detect Max Iterations_condtrue, to: Stop if Max Iterations}
  - {from: Detect Max Iterations_condfalse, to: Generate Prompt, label: feedback, content_type: object}

