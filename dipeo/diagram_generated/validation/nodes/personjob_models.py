# Auto-generated from JSON Schema
# DO NOT EDIT MANUALLY

# generated by datamodel-codegen:
#   filename:  personjob.schema.json
#   timestamp: 2025-08-18T02:04:31+00:00

from __future__ import annotations

from enum import Enum
from typing import Literal, Optional

from pydantic import BaseModel, ConfigDict, Field, RootModel


class MemoryView(Enum):
    """
    Memory management enumerations
    """

    all_involved = 'all_involved'
    sent_by_me = 'sent_by_me'
    sent_to_me = 'sent_to_me'
    system_and_me = 'system_and_me'
    conversation_pairs = 'conversation_pairs'
    all_messages = 'all_messages'


class MemorySettings(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    max_messages: Optional[float] = None
    preserve_system: Optional[bool] = None
    view: MemoryView


class PersonID(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    field__brand: Literal['PersonID'] = Field(..., alias='__brand')


class MemoryProfile(Enum):
    """
    Memory profile: GOLDFISH (2 msgs), MINIMAL (5), FOCUSED (20), FULL (all)
    """

    CUSTOM = 'CUSTOM'
    FOCUSED = 'FOCUSED'
    FULL = 'FULL'
    GOLDFISH = 'GOLDFISH'
    MINIMAL = 'MINIMAL'


class Tools(Enum):
    """
    LLM tools to enable (web_search_preview, etc.)
    """

    image = 'image'
    none = 'none'
    websearch = 'websearch'


class PersonJobNodeData(BaseModel):
    """
    Configuration data for PersonJob nodes that execute LLM agents
    """

    model_config = ConfigDict(
        extra='forbid',
    )
    batch: Optional[bool] = Field(
        None, description='Enable batch processing for arrays'
    )
    batch_input_key: Optional[str] = Field(
        None, description='Array variable name for batch processing'
    )
    batch_parallel: Optional[bool] = Field(
        None, description='Execute batch items in parallel'
    )
    default_prompt: Optional[str] = Field(
        None,
        description='Prompt template using {{variable}} syntax for subsequent iterations',
    )
    first_only_prompt: str = Field(
        ...,
        description='Special prompt for first iteration only, supports {{variable}} syntax',
    )
    first_prompt_file: Optional[str] = Field(
        None,
        description='External prompt file for first iteration only (overrides first_only_prompt)',
    )
    flipped: Optional[bool] = None
    label: str
    max_concurrent: Optional[float] = Field(
        None, description='Maximum concurrent batch executions'
    )
    max_iteration: float = Field(
        ..., description='Maximum conversation turns (default: 1)'
    )
    memory_profile: Optional[MemoryProfile] = Field(
        None,
        description='Memory profile: GOLDFISH (2 msgs), MINIMAL (5), FOCUSED (20), FULL (all)',
    )
    memory_settings: Optional[MemorySettings] = Field(
        None, description='Advanced memory configuration when memory_profile is CUSTOM'
    )
    person: Optional[PersonID] = Field(
        None, description="Reference to agent defined in 'persons' section"
    )
    prompt_file: Optional[str] = Field(
        None,
        description='External prompt file in files/prompts/ (overrides inline prompts)',
    )
    text_format: Optional[str] = Field(
        None, description='Pydantic model name for structured output'
    )
    text_format_file: Optional[str] = Field(
        None,
        description='External Python file with Pydantic models (overrides text_format)',
    )
    tools: Optional[Tools] = Field(
        None, description='LLM tools to enable (web_search_preview, etc.)'
    )


class Model(RootModel[PersonJobNodeData]):
    root: PersonJobNodeData
